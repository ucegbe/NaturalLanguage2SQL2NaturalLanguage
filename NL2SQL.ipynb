{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f851893e-f3cc-4ce6-9b74-c5ac10a436ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "76c3aefd-00b6-47c5-996a-99f44006b85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "# import sentencepiece\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "CLAUDE = Anthropic()\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "import codecs\n",
    "import uuid\n",
    "from transformers import LlamaTokenizer\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "REDSHIFT=boto3.client('redshift-data')\n",
    "S3=boto3.client('s3')\n",
    "from botocore.config import Config\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "config = Config(\n",
    "    read_timeout=600,\n",
    "    retries = dict(\n",
    "        max_attempts = 5\n",
    "    )\n",
    ")\n",
    "BEDROCK=boto3.client(service_name='bedrock-runtime',region_name='us-east-1',config=config)\n",
    "MIXTRAL_ENDPOINT=\"mixtral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7b0a8-f945-44eb-b85d-2364aedf9050",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy Mixtral 8x7 Instruct to SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "raw",
   "id": "965d0bdd-6dc2-4752-a34e-0ded7e972685",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Note this requires an ml.g5.48xlarge instance.\n",
    "model_id = \"huggingface-llm-mixtral-8x7b-instruct\"\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy(endpoint_name=MIXTRAL_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e948f-67b0-4c72-ad33-930071495bed",
   "metadata": {},
   "source": [
    "## REDSHIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de54b-7567-4aee-8ffd-7f5c83ba2397",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a39647d0-ec63-43d6-9043-d34cc8e84141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_counter(path):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(path)\n",
    "    return tokenizer\n",
    "def mixtral_counter(path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "de196317-7d86-425c-9efa-8f496fbe45fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm(prompts,params):   \n",
    "    \"\"\"\n",
    "    Function to prompt the model to generate sql statements from natural language\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import json\n",
    "    if \"claude\" in params[\"sql_model\"].lower():\n",
    "        prompt={\n",
    "          \"prompt\": prompts,\n",
    "          \"max_tokens_to_sample\": params['sql-token'],\n",
    "          \"temperature\": params['temp'],\n",
    "          # \"top_k\": 50,\n",
    "          # \"top_p\": 1,  \n",
    "             \"stop_sequences\": []\n",
    "        }\n",
    "        prompt=json.dumps(prompt)\n",
    "        output = BEDROCK.invoke_model(body=prompt,\n",
    "                                    modelId=params['sql_model'], \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "        output=output['body'].read().decode() \n",
    "        answer=json.loads(output)['completion']\n",
    "        return answer\n",
    "    elif \"mixtral\" in params[\"sql_model\"].lower():\n",
    "       \n",
    "        payload = {\n",
    "            \"inputs\":prompts,\n",
    "            \"parameters\": {\"max_new_tokens\": params['sql-token'],\n",
    "                           # \"top_p\": params['top_p'], \n",
    "                           \"temperature\":params['temp'],\n",
    "                           \"return_full_text\": False,}\n",
    "        }\n",
    "        llama=boto3.client(\"sagemaker-runtime\")\n",
    "        output=llama.invoke_endpoint(Body=json.dumps(payload), EndpointName=MIXTRAL_ENDPOINT,ContentType=\"application/json\")\n",
    "        answer=json.loads(output['Body'].read().decode())[0]['generated_text']  \n",
    "        return answer\n",
    "\n",
    "\n",
    "def qna_llm(prompts,params):\n",
    "    \"\"\"\n",
    "    Function to prompt the model to generate natural language answers from sql results\n",
    "    \"\"\"\n",
    "    import json    \n",
    "    if 'claude' in params['text-model'].lower():        \n",
    "\n",
    "        inference_modifier = { \"max_tokens_to_sample\": round(params['token']),\n",
    "          # \"temperature\": 0.5, \n",
    "             \"stop_sequences\": []        \n",
    "                     }       \n",
    "        prompts\n",
    "        llm = Bedrock(model_id=params['text-model'], client=BEDROCK, model_kwargs = inference_modifier,streaming=True,  callbacks=[StreamingStdOutCallbackHandler()]) \n",
    "        answer =llm.invoke(prompts)      \n",
    "\n",
    "    elif 'mixtral' in params['text-model'].lower():        \n",
    "        import boto3\n",
    "        import json\n",
    "        payload = {\n",
    "            \"inputs\":prompts,\n",
    "            \"parameters\": {\"max_new_tokens\": params['token'], \n",
    "                           # \"top_p\": params['top_p'], \n",
    "                           \"temperature\": params['temp'],\n",
    "                           \"return_full_text\": False,}\n",
    "        }\n",
    "        llama=boto3.client(\"sagemaker-runtime\")\n",
    "        output=llama.invoke_endpoint(Body=json.dumps(payload), EndpointName=MIXTRAL_ENDPOINT,ContentType=\"application/json\")\n",
    "        answer=json.loads(output['Body'].read().decode())[0]['generated_text']   \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "dec641b4-70a4-45d3-86cb-bd876d15e354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_csv_rows(csv_rows, max_token_per_chunk):\n",
    "    \"\"\"\n",
    "    Chunk CSV rows based on the maximum token count per chunk.\n",
    "    Args:\n",
    "        csv_rows (list): List of CSV rows.\n",
    "        max_token_per_chunk (int, optional): Maximum token count per chunk.\n",
    "    Returns:\n",
    "        list: List of chunks containing CSV rows.\n",
    "    Raises:\n",
    "        ValueError: If a single CSV row exceeds the specified max_token_per_chunk.\n",
    "    \"\"\"\n",
    "    header = csv_rows[0]  # Assuming the first row is the header\n",
    "    csv_rows = csv_rows[1:]  # Remove the header from the list\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    chunks = []\n",
    "    header_token=len(mixtral_counter(\"mistralai/Mixtral-8x7B-v0.1\").encode(header))\n",
    "    for row in csv_rows:\n",
    "        token = len(mixtral_counter(\"mistralai/Mixtral-8x7B-v0.1\").encode(row))\n",
    "        if current_token_count + token+header_token <= max_token_per_chunk:\n",
    "            current_chunk.append(row)\n",
    "            current_token_count += token\n",
    "        else:\n",
    "            if not current_chunk:\n",
    "                raise ValueError(\"A single CSV row exceeds the specified max_token_per_chunk.\")\n",
    "            header_and_chunk=[header]+current_chunk\n",
    "            chunks.append(\"\\n\".join([x for x in header_and_chunk]))\n",
    "            current_chunk = [row]\n",
    "            current_token_count = token\n",
    "\n",
    "    if current_chunk:\n",
    "        last_chunk_and_header=[header]+current_chunk\n",
    "        chunks.append(\"\\n\".join([x for x in last_chunk_and_header]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "248fc3be-eede-4b2c-a926-8d5c7a04f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_redshift(identifier, database,  schema,serverless,db_user=None,):\n",
    "    \"\"\"\n",
    "    Get a list of table names in a specified schema from an Amazon Redshift cluster.\n",
    "    Args:\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database containing the tables.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "        schema (str): The schema pattern to filter tables.\n",
    "    Returns:\n",
    "        list: A list of table names in the specified schema.\n",
    "    \"\"\"\n",
    "    if serverless:\n",
    "        tables_ls = REDSHIFT.list_tables(\n",
    "       WorkgroupName=identifier,\n",
    "        Database=database,\n",
    "        SchemaPattern=schema\n",
    "        )\n",
    "    else:\n",
    "        tables_ls = REDSHIFT.list_tables(\n",
    "        ClusterIdentifier=identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        SchemaPattern=schema\n",
    "        )\n",
    "    return [x['name'] for x in  tables_ls['Tables']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "47e81566-6e5e-4771-9fdd-65343c388f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_redshift(identifier, database,serverless, db_user=None, ):\n",
    "    \"\"\"\n",
    "    Get a list of databases from an Amazon Redshift cluster.\n",
    "    Args:\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database containing the tables.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "    Returns:\n",
    "        list: A list of databases in the Redshift cluster.\n",
    "    \"\"\"\n",
    "    if serverless:\n",
    "        db_ls = REDSHIFT.list_databases(\n",
    "        WorkgroupName=identifier,\n",
    "        Database=database,\n",
    "        )    \n",
    "    else:\n",
    "        db_ls = REDSHIFT.list_databases(\n",
    "        ClusterIdentifier=identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user\n",
    "        )\n",
    "    return db_ls['Databases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "34cf5577-7130-4302-a03f-4f5a60db0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_redshift(identifier, database, serverless, db_user=None,):\n",
    "    \"\"\"\n",
    "    Get a list of schemas from an Amazon Redshift cluster.\n",
    "    Args:\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database containing the schemas.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "    Returns:\n",
    "        list: A list of schemas in the Redshift cluster.\n",
    "    \"\"\"\n",
    "    if serverless:\n",
    "        schema_ls = REDSHIFT.list_schemas(\n",
    "        WorkgroupName=identifier,\n",
    "        Database=database,\n",
    "\n",
    "        )\n",
    "    else:        \n",
    "        schema_ls = REDSHIFT.list_schemas(\n",
    "        ClusterIdentifier=identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user\n",
    "        )\n",
    "    return schema_ls['Schemas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "70f38a84-47bb-481e-a348-4b32ca267b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query_with_pagination( sql_query, identifier, database,  serverless, db_user=None,):\n",
    "    \"\"\"\n",
    "    Execute multiple SQL queries in Amazon Redshift with pagination support.\n",
    "    Args:\n",
    "        sql_query1 (str): The first SQL query to execute.\n",
    "        sql_query2 (str): The second SQL query to execute.\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "    Returns:\n",
    "        list: A list of results from executing the SQL queries.\n",
    "    \"\"\"\n",
    "    results_list=[]\n",
    "    if serverless:\n",
    "        response_b = REDSHIFT.batch_execute_statement(\n",
    "            WorkgroupName=identifier,\n",
    "            Database=database,\n",
    "        \n",
    "            Sqls=sql_query\n",
    "        ) \n",
    "    else:\n",
    "        response_b = REDSHIFT.batch_execute_statement(\n",
    "            ClusterIdentifier=identifier,\n",
    "            Database=database,\n",
    "            DbUser=db_user,\n",
    "            Sqls=sql_query\n",
    "        )   \n",
    "    describe_b=REDSHIFT.describe_statement(\n",
    "         Id=response_b['Id'],\n",
    "    )       \n",
    "    status=describe_b['Status']\n",
    "    while status != \"FINISHED\":\n",
    "        time.sleep(1)\n",
    "        describe_b=REDSHIFT.describe_statement(\n",
    "                         Id=response_b['Id'],\n",
    "                    ) \n",
    "        status=describe_b['Status']\n",
    "    max_attempts = 5 \n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            for ids in describe_b['SubStatements']:\n",
    "                result_b = REDSHIFT.get_statement_result(Id=ids['Id'])                \n",
    "                results_list.append(get_redshift_table_result(result_b))\n",
    "            break\n",
    "        except REDSHIFT.exceptions.ResourceNotFoundException as e:\n",
    "            attempts += 1\n",
    "            time.sleep(2)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "bb1c043c-60d0-4d34-8fdd-886d6120c40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_redshift_table_result(response):\n",
    "    \"\"\"\n",
    "    Extracts result data from a Redshift query response and returns it as a CSV string.\n",
    "    Args:\n",
    "        response (dict): The response object from a Redshift query.\n",
    "    Returns:\n",
    "        str: A CSV string containing the result data.\n",
    "    \"\"\"\n",
    "    columns = [c['name'] for c in response['ColumnMetadata']] \n",
    "    data = []\n",
    "    for r in response['Records']:\n",
    "        row = []\n",
    "        for col in r:\n",
    "            row.append(list(col.values())[0])  \n",
    "        data.append(row)\n",
    "    df = pd.DataFrame(data, columns=columns)    \n",
    "    return df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8c24a0b9-ed77-4222-bc87-af828502e2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query_redshyft(sql_query, identifier, database, serverless,db_user=None):\n",
    "    \"\"\"\n",
    "    Execute a SQL query on an Amazon Redshift cluster.\n",
    "    Args:\n",
    "        sql_query (str): The SQL query to execute.\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "    Returns:\n",
    "        dict: The response object from executing the SQL query.\n",
    "    \"\"\"\n",
    "    if serverless:\n",
    "        response = REDSHIFT.execute_statement(\n",
    "        WorkgroupName=identifier,\n",
    "            Database=database,\n",
    "\n",
    "            Sql=sql_query\n",
    "        )\n",
    "    else:        \n",
    "        response = REDSHIFT.execute_statement(\n",
    "            ClusterIdentifier=identifier,\n",
    "            Database=database,\n",
    "            DbUser=db_user,\n",
    "            Sql=sql_query\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1a7c4ef0-9166-4faf-a820-0c03d8974e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_execute_query(params,sql_query, identifier, database, question,serverless,db_user=None):\n",
    "    \"\"\"\n",
    "    Execute a single SQL query on an Amazon Redshift cluster and process the result.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): The SQL query to execute.\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "        question (str): A descriptive label or question associated with the query.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the processed result of the SQL query.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = execute_query_redshyft(sql_query, identifier, database,serverless, db_user)\n",
    "    df=redshyft_querys(sql_query,response,question,params,identifier, \n",
    "                       database,                     \n",
    "                       question,\n",
    "                         db_user,)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "30c4b2f5-65e8-4c9c-b0bc-f19406d7838c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_debugga(question, statement, error, params): \n",
    "    \"\"\"\n",
    "    Generate debugging guidance and expected SQL correction for a PostgreSQL error.\n",
    "    Args:\n",
    "        question (str): The user's question or intent.\n",
    "        statement (str): The SQL statement that caused the error.\n",
    "        error (str): The error message encountered.\n",
    "        params (dict): Additional parameters including schema, sample data, and length.\n",
    "    Returns:\n",
    "        str: Formatted debugging guidance and expected SQL correction.\n",
    "    \"\"\"\n",
    "    print(error)\n",
    "    model=\"claude\" if \"claude\" in params[\"sql_model\"].lower() else \"mixtral\" \n",
    "    with open(f\"prompt/{params['prompt-type']}/{model}-debugger.txt\",\"r\") as f:\n",
    "        prompts=f.read()\n",
    "    values = {\n",
    "    \"error\":error,\n",
    "    \"sql\":statement,\n",
    "    \"schema\": params['schema'],\n",
    "    \"sample\": params['sample'],\n",
    "    \"question\":params['prompt']\n",
    "    }\n",
    "    prompts=prompts.format(**values)\n",
    "    if \"claude\" == model:\n",
    "        prompts=f\"\\n\\nHuman: {prompts}\\n\\nAssistant:\"    \n",
    "    params['prompta']=prompts\n",
    "    answer=query_llm(prompts,params)\n",
    "    print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "aae1ae4a-1171-48c4-9799-c0d57566015b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def redshyft_querys(q_s,response,prompt,params,identifier, database, question,db_user=None,): \n",
    "    \"\"\"\n",
    "    Execute a Redshift query, handle errors, debug SQL, and return the result.\n",
    "\n",
    "    Args:\n",
    "        q_s (str): The SQL statement to execute or debug.\n",
    "        response (dict): The response object from executing the SQL statement.\n",
    "        prompt (str): The user's question or intent.\n",
    "        params (dict): Additional parameters including schema, sample data, and length.\n",
    "        identifier (str): The identifier of the Redshift cluster.\n",
    "        database (str): The name of the database.\n",
    "        db_user (str): The username used to authenticate with the Redshift cluster.\n",
    "        question (str): A descriptive label or question associated with the query.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame or str: DataFrame containing the query result, or debugging failure message with no result.\n",
    "\n",
    "    \"\"\"\n",
    "    max_execution=5\n",
    "    debug_count=max_execution\n",
    "    alert=False\n",
    "    try:\n",
    "        statement_result = REDSHIFT.get_statement_result(\n",
    "            Id=response['Id'],\n",
    "        )\n",
    "    except REDSHIFT.exceptions.ResourceNotFoundException as err:  \n",
    "        # print(err)\n",
    "        describe_statement=REDSHIFT.describe_statement(\n",
    "             Id=response['Id'],\n",
    "        )\n",
    "        query_state=describe_statement['Status']  \n",
    "        while query_state in ['SUBMITTED','PICKED','STARTED']:\n",
    "            # print(query_state)\n",
    "            time.sleep(1)\n",
    "            describe_statement=REDSHIFT.describe_statement(\n",
    "                 Id=response['Id'],\n",
    "            )\n",
    "            query_state=describe_statement['Status']\n",
    "        while (max_execution > 0 and query_state == \"FAILED\"):\n",
    "            max_execution = max_execution - 1\n",
    "            print(f\"\\nDEBUG TRIAL {max_execution}\")\n",
    "            bad_sql=describe_statement['QueryString']\n",
    "            print(f\"\\nBAD SQL:\\n{bad_sql}\")                \n",
    "            error=describe_statement['Error']\n",
    "            print(f\"\\nERROR:{error}\")\n",
    "            print(\"\\nDEBUGGIN...\")\n",
    "            cql=llm_debugga(prompt, bad_sql, error, params)            \n",
    "            idx1 = cql.index('<sql>')\n",
    "            idx2 = cql.index('</sql>')\n",
    "            q_s=cql[idx1 + len('<sql>') + 1: idx2]\n",
    "            print(f\"\\nDEBUGGED SQL\\n {q_s}\")\n",
    "            ### Guardrails to prevent the LLM from altering tables\n",
    "            if any(keyword in q_s for keyword in [\"CREATE\", \"DROP\", \"ALTER\",\"INSERT\",\"UPDATE\",\"TRUNCATE\",\"DELETE\",\"MERGE\",\"REPLACE\",\"UPSERT\"]):\n",
    "                alert=\"I AM NOT PERMITTED TO MODIFY THIS TABLE, CONTACT ADMIN.\"\n",
    "                print(alert)\n",
    "                alert=True\n",
    "                break\n",
    "            else:\n",
    "                response = execute_query_redshyft(q_s, identifier, database,params['serverless'],db_user)\n",
    "                describe_statement=REDSHIFT.describe_statement(\n",
    "                                     Id=response['Id'],\n",
    "                                )\n",
    "                query_state=describe_statement['Status']\n",
    "                while query_state in ['SUBMITTED','PICKED','STARTED']:\n",
    "                    time.sleep(2)            \n",
    "                    describe_statement=REDSHIFT.describe_statement(\n",
    "                                     Id=response['Id'],\n",
    "                                )\n",
    "                    query_state=describe_statement['Status']\n",
    "                if query_state == \"FINISHED\":                \n",
    "                    break \n",
    "        \n",
    "        if max_execution == 0 and query_state == \"FAILED\":\n",
    "            print(f\"DEBUGGING FAILED IN {str(debug_count)} ATTEMPTS\")\n",
    "        elif alert:\n",
    "            pass\n",
    "        else:           \n",
    "            max_attempts = 5\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    statement_result = REDSHIFT.get_statement_result(\n",
    "                        Id=response['Id']\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                except REDSHIFT.exceptions.ResourceNotFoundException as e:\n",
    "                    attempts += 1\n",
    "                    time.sleep(5)\n",
    "    if max_execution == 0 and query_state == \"FAILED\":\n",
    "        df=f\"DEBUGGING FAILED IN {str(debug_count)} ATTEMPTS. NO RESULT AVAILABLE\"\n",
    "    elif alert:\n",
    "        df=\"I AM NOT PERMITTED TO MODIFY THIS TABLE, CONTACT ADMIN.\"     \n",
    "    else:\n",
    "        df=get_redshift_table_result(statement_result)\n",
    "    return df, q_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3cab616f-b529-4646-961f-0bb1e420e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redshift_qna(params):\n",
    "    \"\"\"\n",
    "    Execute a Q&A process for generating SQL queries based on user questions.\n",
    "    Args:\n",
    "        params (dict): A dictionary containing parameters including table name, database name, prompt, etc.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the response, generated SQL statement, and query output.\n",
    "    \"\"\"\n",
    "    sql1=f\"SELECT table_catalog,table_schema,table_name,column_name,ordinal_position,is_nullable,data_type FROM information_schema.columns WHERE table_schema='{params['db-schema']}'\"\n",
    "    sql2=[]\n",
    "    for table in params['tables']:\n",
    "        sql2.append(f\"SELECT * from {params['database']}.{params['db-schema']}.{table} LIMIT 3\")\n",
    "    sqls=[sql1]+sql2\n",
    "    \n",
    "    question=params['prompt']\n",
    "    results=execute_query_with_pagination(sqls, IDENTIFIER, params['database'],params['serverless'], DB_USER)    \n",
    "    # print(results)\n",
    "    col_names=results[0].split('\\n')[0]\n",
    "    \n",
    "    observations=\"\\n\".join(sorted(results[0].split('\\n')[1:])).strip()\n",
    "\n",
    "    params['schema']=f\"{col_names}\\n{observations}\"\n",
    "    \n",
    "    params['sample']=''\n",
    "    for examples in results[1:]:\n",
    "        params['sample']+=f\"{examples}\\n\"\n",
    "\n",
    "    model=\"claude\" if \"claude\" in params[\"sql_model\"].lower() else \"mixtral\"\n",
    "    with open(f\"prompt/{params['prompt-type']}/{model}-sql.txt\",\"r\") as f:\n",
    "        prompts=f.read()\n",
    "    values = {\n",
    "    \"schema\": params['schema'],\n",
    "    \"sample\": params['sample'],\n",
    "    \"question\": question,\n",
    "    }\n",
    "    prompts=prompts.format(**values)\n",
    "    if \"claude\" == model:\n",
    "        prompts=f\"\\n\\nHuman: {prompts}\\n\\nAssistant:\"  \n",
    "    \n",
    "    q_s=query_llm(prompts,params)\n",
    "    sql_pattern = re.compile(r'<sql>(.*?)(?:</sql>|$)', re.DOTALL)           \n",
    "    sql_match = re.search(sql_pattern, q_s)\n",
    "    q_s = sql_match.group(1) \n",
    "    print(f\" FIRST ATTEMPT SQL:\\n{q_s}\")\n",
    "    ### Guardrails to prevent the LLM from altering tables\n",
    "    if any(keyword in q_s for keyword in [\"CREATE\", \"DROP\", \"ALTER\",\"INSERT\",\"UPDATE\",\"TRUNCATE\",\"DELETE\",\"MERGE\",\"REPLACE\",\"UPSERT\"]):\n",
    "        output=\"I AM NOT PERMITTED TO MODIFY THIS TABLE, CONTACT ADMIN.\"\n",
    "        response=\"I AM NOT PERMITTED TO MODIFY THIS TABLE, CONTACT ADMIN.\"\n",
    "    else:\n",
    "        output, q_s=single_execute_query(params,q_s, IDENTIFIER, params['database'], question, params['serverless'],DB_USER) \n",
    "        # Handle results that exceed LLM token window length\n",
    "        input_token=CLAUDE.count_tokens(output) if \"claude\" in params['text-model'].lower() else mistral_counter(\"mistralai/Mixtral-8x7B-v0.1\").encode(output)\n",
    "        if (\"claude\" in params['text-model'].lower() and input_token>90000) or (\"claude\" not in params['text-model'].lower() and len(input_token)>28000):    \n",
    "            csv_rows=output.split('\\n')\n",
    "            chunk_rows=chunk_csv_rows(csv_rows, 20000)\n",
    "            initial_summary=[]\n",
    "            for chunk in chunk_rows:\n",
    "                model=\"claude\" if \"claude\" in params['text-model'].lower() else \"mixtral\"\n",
    "                with open(f\"prompt/{params['prompt-type']}/{model}-text-gen.txt\",\"r\") as f:\n",
    "                    prompts=f.read()\n",
    "                values = {   \n",
    "                \"sql\":q_s,\n",
    "                \"csv\": output,       \n",
    "                \"question\":question,\n",
    "                }\n",
    "                prompts=prompts.format(**values)\n",
    "                if \"claude\" == model:\n",
    "                    prompts=f\"\\n\\nHuman:\\n{prompts}\\n\\nAssistant:\"\n",
    "                initial_summary.append(qna_llm(prompts,params))\n",
    "            prompts = f'''You are a helpful and truthful assistant.\n",
    "Here are multiple answer for a question on different subset of a tabular data:\n",
    "#######\n",
    "{initial_summary}\n",
    "#######\n",
    "Question: {question}\n",
    "Based on the given question above, merege all answers provided in a coherent singular answer'''\n",
    "            if \"claude\" == model:\n",
    "                prompts=f\"\\n\\nHuman: {prompts}\\n\\nAssistant:\"\n",
    "            response=qna_llm(prompts,params)\n",
    "\n",
    "        else:        \n",
    "            model=\"claude\" if \"claude\" in params['text-model'].lower() else \"mixtral\"\n",
    "            with open(f\"prompt/{params['prompt-type']}/{model}-text-gen.txt\",\"r\") as f:\n",
    "                prompts=f.read()\n",
    "            values = {   \n",
    "            \"sql\":q_s,\n",
    "            \"csv\": output,       \n",
    "            \"question\":question,\n",
    "            }\n",
    "            prompts=prompts.format(**values)\n",
    "            if \"claude\" == model:\n",
    "                prompts=f\"\\n\\nHuman: {prompts}\\n\\nAssistant:\"\n",
    "            response=qna_llm(prompts, params) \n",
    "    return response, q_s,output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0427b27-bb38-4cce-9054-ab51507a37f1",
   "metadata": {},
   "source": [
    "#### Change parameters below to those of your Redshift resource\n",
    "For Redshift provisioned capacity set `IDENTIFIER` to your cluster-identifier for your redshift cluster and `DB_USER` to admin username and set `serverless` to False \\\n",
    "For redshidft serverless, set `IDENTIFIER` to your work-group name and set `serverless` to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f69fcae3-32f0-4048-a2cf-74183b928182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are Tables within this Database: sample_data_dev and Schema: tickit\n",
      " Tables: ['category', 'date', 'event', 'listing', 'sales', 'users', 'venue']\n"
     ]
    }
   ],
   "source": [
    "IDENTIFIER = 'test'#'redshift-cluster-llm'\n",
    "DATABASE = 'dev'\n",
    "serverless=True\n",
    "DB_USER = 'faynxe' if not serverless else None\n",
    "#View the list of database and tables in your redshift cluster\n",
    "db=get_db_redshift(IDENTIFIER, DATABASE,serverless,DB_USER)[-2] #Get list of Database\n",
    "schm=get_schema_redshift(IDENTIFIER, db,serverless,DB_USER)[-1] # Get list of Database Schema\n",
    "tables=get_tables_redshift(IDENTIFIER, db,schm,serverless,DB_USER) # Get list of tables in a database schema\n",
    "\n",
    "print(f\" Here are Tables within this Database: {db} and Schema: {schm}\\n Tables: {tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c4a5e-086b-453a-aa33-bd9188409317",
   "metadata": {},
   "source": [
    "You can decide the number of tables to be loaded into the LLM prompt for query. For each table selected, the schema definition and sample rows are loaded into the LLM prompt.\n",
    "For single table query, you can pass the single table name in the params definition below for a given Database Schema.\n",
    "For query that requires insight into multiple tables, pass the list of tables in the params definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "536466a5-80a8-4a69-bc8e-b390cccc2432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "question=\"20 examples of 2 friends' names each who attended the same event together?\"\n",
    "params={'sql-token':700,'token':500,'tables':tables, # Tables must be in list format\n",
    "        'db-schema':schm, # Redshift Database SchemaName,\n",
    "        \"database\":db,\n",
    "        'temp':0.5,\n",
    "        \"serverless\": serverless, # Toggle this to True if using Redshift Serverless\n",
    "        'text-model':'anthropic.claude-instant-v1', # SQL to NL model\n",
    "        \"prompt-type\":\"redshift\",\n",
    "        \"sql_model\":\"mixtral\",#\"anthropic.claude-v2:1\",'anthropic.claude-instant-v1',\"mixtral\" NL to SQL model\n",
    "        \"prompt\":question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "642ae516-41e5-453e-be1a-ca7de1c1e9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint mixtral of account 715253196401 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[272], line 40\u001b[0m, in \u001b[0;36mredshift_qna\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m model:\n\u001b[1;32m     38\u001b[0m     prompts\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m---> 40\u001b[0m q_s\u001b[38;5;241m=\u001b[39m\u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m sql_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sql>(.*?)(?:</sql>|$)\u001b[39m\u001b[38;5;124m'\u001b[39m, re\u001b[38;5;241m.\u001b[39mDOTALL)           \n\u001b[1;32m     42\u001b[0m sql_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(sql_pattern, q_s)\n",
      "Cell \u001b[0;32mIn[261], line 34\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(prompts, params)\u001b[0m\n\u001b[1;32m     26\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompts,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql-token\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_full_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,}\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     33\u001b[0m llama\u001b[38;5;241m=\u001b[39mboto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker-runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMIXTRAL_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m answer\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint mixtral of account 715253196401 not found."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ress=redshift_qna(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7a388-c7dd-40df-bda1-3719044129d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Result and SQL Statement\n",
    "print(f\"Answer:\\n{ress[0]}\\n\\nSQL:\\n{ress[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "884c31f9-917e-45a3-94fb-1c05eef677b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>total_tickets_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSA50YZS</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PFV99JWI</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMO84OWJ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ITR41VBX</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EYS67OKG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   username  total_tickets_sold\n",
       "0  LSA50YZS                   6\n",
       "1  PFV99JWI                   5\n",
       "2  IMO84OWJ                   4\n",
       "3  ITR41VBX                   4\n",
       "4  EYS67OKG                   4"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query Result\n",
    "df=pd.read_csv(StringIO(ress[2]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab392c4-be7a-42dd-b493-52ddb2a8c178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
